---
title: "Homework 5"
author: "Tongtong Zhu"
date: "2022-11-12"
output: github_document
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 8,
  fig.height = 6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```


## Problem 1

### Create a tidy dataframe and manipulate

```{r, message=FALSE}
longi_df =
  tibble(
    file = list.files("data")) %>% 
  mutate(
    path = str_c("data/", file),
    data = purrr::map(.x = path, ~read_csv(.x))) %>% 
  unnest(data) %>% 
  mutate(id = str_extract(file, "\\d+"),
         id = as.numeric(id),
         arm = str_extract(file, "con|exp")) %>% 
  relocate(id, arm, path,everything()) %>% 
  mutate(arm = recode(arm, con = "control", exp = "experimental")) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observation"
  )

longi_df
```

### Make a spaghetti plot

```{r}
longi_df %>% 
  ggplot(aes(x = week, y = observation, group = id, color = arm)) +
  geom_path() +
  labs(
      x = "Week",
      y = "Observation",
      title = "Observations for subjects in experimental and control groups for each week"
      )
```

**Comment on difference**

In general, the observation values of the experimental group continue to increase, while the observation values of the control group are relatively stable over time. On average, the observation values of the experimental group are higher than those of the control group. The difference of observations between two groups gradually increases from week 2.

## Problem 2

### Describe the raw data

```{r, message = FALSE}
homicide_raw = read_csv("./data_homi/homicide-data.csv")
```

The `homicide_raw` dataset contains information collected by the *Washington Post* on criminal homicides over the past decade in 50 of the largest US cities. It contains `r nrow(homicide_raw)` observations and `r ncol(homicide_raw)` variables. The key variables include `r colnames(homicide_raw)`.

### Create `city_state` and `resolution` variables 

```{r}
homicide_df =
  homicide_raw %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    city_state = recode(city_state, "Tulsa, AL" = "Tulsa, OK"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )) %>% 
  relocate(city_state)
      
```

### Summarize total No. of homicides and unsolved homicides within cities

```{r}
homi_summary =
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    total_homi = n(),
    homi_unsolved = sum(resolved == "unsolved")
  )

homi_summary
```

### Estimate the proportion of unsolved homicides for Baltimore

```{r}
balt_prop = 
  prop.test(
    x = homi_summary %>% filter(city_state == "Baltimore, MD") %>% pull(homi_unsolved),
    n = homi_summary %>% filter(city_state == "Baltimore, MD") %>% pull(total_homi)
  )  

balt_prop %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high)
```

### Estimate proportion and CI for each city

```{r}
cities_prop = 
  homi_summary %>%
  mutate(
    prop_test = map2(.x = homi_unsolved, .y = total_homi, ~prop.test(x = .x, n = .y)),
    tidy_test = map(.x = prop_test, ~broom::tidy(.x))
  ) %>% 
  select(city_state, tidy_test) %>% 
  unnest(tidy_test) %>% 
  select(city_state, estimate, conf.low, conf.high)

cities_prop
  
```

### Create a plot showing estimates and CIs

```{r}
cities_prop %>% 
  mutate(
    city_state = fct_reorder(city_state, estimate)
  ) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
   labs(
    x = "City, State",
    y = "Proportion of unsolved homicides",
    title = "Estimated Proportions of Unsolved Homicides with 95% CIs for Each City"
   )
```

## Problem 3

### Create a function with default vaule

```{r}

sim_t_test = function(n = 30, mu, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma),
  )
  
  sim_data %>% 
    t.test() %>% 
    broom::tidy() %>%
    select(estimate, p.value)
    
}

```

### Generate 5000 datasets from the model with mu=0 

```{r}
sim_results_mu0 =
  expand_grid(
    mu_value = 0,
    iter = 1:5000
  ) %>% 
  mutate(
    estimate_df = map(.x = mu_value, ~sim_t_test(mu = .x))
  ) %>% 
  unnest(estimate_df)
```


### Repeat the model with several mu={1,2,3,4,5,6} 

```{r}
sim_results_mu6 =
  expand_grid(
    mu_value = 1:6,
    iter = 1:5000
  ) %>% 
  mutate(
    estimate_df = map(.x = mu_value, ~sim_t_test(mu = .x))
  ) %>% 
  unnest(estimate_df)
```

### Make a plot showing the proportion of null-rejected

```{r}
sim_results_mu6 %>% 
  group_by(mu_value) %>% 
  summarize(
    total = n(),
    null_reject = sum(p.value < 0.05)) %>% 
  mutate(
    proportion = null_reject / total
  ) %>%
  ggplot(aes(x = mu_value, y = proportion)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Proportion of null hypothesis rejected for each μ value",
    x = "True value of μ "
  ) +
  scale_x_continuous(
    breaks = 1:6
  )
```

**Description of the association between effect size and power**

The plot shows that as the true value of μ increases, the proportion of null hypothesis rejected increases, that is, the power increases. As mentioned in the null hypothesis, the mean equals 0, so as the true mean increases, the effect size increases. Therefore, the larger the effect size, the more power the test has.

### Make a plot showing the average estimate of μ vs. its true value 

```{r}
all_estimate =
  sim_results_mu6 %>% 
  group_by(mu_value) %>% 
  summarize(
    avg_estimate = mean(estimate)
  ) 
```

```{r}
rejected_estimate =
sim_results_mu6 %>% 
  filter(p.value < 0.05) %>% 
  group_by(mu_value) %>% 
  summarize(
    avg_estimate = mean(estimate)
  ) 
```


```{r}
all_estimate %>% 
  ggplot(aes(x = mu_value, y = avg_estimate)) +
  geom_line(aes(color = "a"), alpha = 0.5) +
  geom_line(data = rejected_estimate, aes(color = "b"), alpha = 0.5) +
  scale_color_manual(name = " ", 
                     values = c("a" = "red","b" = "blue"), 
                     labels = c("all samples","rejected null samples")) +
  geom_point(color = "red") +
  geom_point(data = rejected_estimate, color = "blue") +
  scale_x_continuous(breaks = 1:6) +
  scale_y_continuous(breaks = 1:6) +
   labs(
     title = "Average estimate of mean for each true value of mean",
     x = "True value of mean",
     y = "Average estimate of mean"
   )
  

```


**Description of the plot**

When true mean μ ={1,2,3}, the sample average of mean across tests for which the null is rejected is not approximately equal to the true mean. When the true mean μ ={4,5,6}, the sample average of mean (null rejected) is approximately equal to the true mean. 

It is because smaller effect size corresponds to smaller power. We reject the null hypothesis when the estimated values of samples are significantly different from the null (μ=0). When the true mean is less than 4, the effect size is relatively small and the power is not big enough to find all the statistically significant difference between samples and true values. So when the true mean is less than 4, not all samples with significant difference are rejected, and thus the sample average of mean (for which the null is rejected) is different with the true mean. However, when the true mean is equal to or larger than 4, the effect size is larger and the power is big enough to find all the statistically significant difference and almost reject all the samples in this case. Therefore, the sample average of mean (for which the null is rejected) equals to the true mean.





