---
title: "Homework 5"
author: "Tongtong Zhu"
date: "2022-11-12"
output: github_document
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```


## Problem 1

### Create a tidy dataframe and manipulate

```{r,show_col_types = FALSE}
longi_df =
  tibble(
    file = list.files("data")) %>% 
  mutate(
    path = str_c("data/", file),
    data = purrr::map(.x = path, ~read_csv(.x))) %>% 
  unnest(data) %>% 
  mutate(id = str_extract(file, "\\d+"),
         id = as.numeric(id),
         arm = str_extract(file, "con|exp")) %>% 
  relocate(id, arm, path,everything()) %>% 
  mutate(arm = recode(arm, con = "control", exp = "experimental")) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observation"
  )
```

### Make a spaghetti plot

```{r}
longi_df %>% 
  ggplot(aes(x = week, y = observation, group = id, color = arm)) +
  geom_path() +
  labs(
      x = "Week",
      y = "Observation",
      title = "Observations for subjects in experimental and control groups for each week"
      )
```

**Comment on difference**

In general, the observation values of the experimental group continue to increase, while the observation values of the control group are relatively stable over time. On average, the observation values of the experimental group are higher than those of the control group. The difference of observations between two groups gradually increases from week 2.

## Problem 2

### Describe the raw data

```{r, message = FALSE}
homicide_raw = read_csv("./data_homi/homicide-data.csv")
```

The `homicide_raw` dataset contains information collected by the *Washington Post* on criminal homicides over the past decade in 50 of the largest US cities. It contains `r nrow(homicide_raw)` observations and `r ncol(homicide_raw)` variables. The key variables include `r colnames(homicide_raw)`.

### Create `city_state` and `resolution` variables 

```{r}
homicide_df =
  homicide_raw %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    city_state = recode(city_state, "Tulsa, AL" = "Tulsa, OK"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )) %>% 
  relocate(city_state)
      
```

### Summarize total No. of homicides and unsolved homicides within cities

```{r}
homi_summary =
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    total_homi = n(),
    homi_unsolved = sum(resolved == "unsolved")
  )

homi_summary
```

### Estimate the proportion of unsolved homicides for Baltimore

```{r}
balt_prop = 
  prop.test(
    x = homi_summary %>% filter(city_state == "Baltimore, MD") %>% pull(homi_unsolved),
    n = homi_summary %>% filter(city_state == "Baltimore, MD") %>% pull(total_homi)
  )  

balt_prop %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high)
```

### Estimate proportion and CI for each city

```{r}
cities_prop = 
  homi_summary %>%
  mutate(
    prop_test = map2(.x = homi_unsolved, .y = total_homi, ~prop.test(x = .x, n = .y)),
    tidy_test = map(.x = prop_test, ~broom::tidy(.x))
  ) %>% 
  select(city_state, tidy_test) %>% 
  unnest(tidy_test) %>% 
  select(city_state, estimate, conf.low, conf.high)

cities_prop
  
```

### Create a plot showing estimates and CIs

```{r}
cities_prop %>% 
  mutate(
    city_state = fct_reorder(city_state, estimate)
  ) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
   labs(
    x = "City, State",
    y = "Proportion of unsolved homicides",
    title = "Estimated Proportions of Unsolved Homicides with 95% CIs for Each City"
   )
```

## Problem 3

### Create a function with default vaule

```{r}

sim_t_test = function(n = 30, mu, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma),
  )
  
  sim_data %>% 
    t.test() %>% 
    broom::tidy() %>%
    select(estimate, p.value)
    
}

```

### Generate 5000 datasets from the model with mu=0 

```{r}
sim_results_mu0 =
  expand_grid(
    mu_value = 0,
    iter = 1:5000
  ) %>% 
  mutate(
    estimate_df = map(.x = mu_value, ~sim_t_test(mu = .x))
  ) %>% 
  unnest(estimate_df)
```


### Repeat the model with several mu={1,2,3,4,5,6} 

```{r}
sim_results_mu6 =
  expand_grid(
    mu_value = 1:6,
    iter = 1:5000
  ) %>% 
  mutate(
    estimate_df = map(.x = mu_value, ~sim_t_test(mu = .x))
  ) %>% 
  unnest(estimate_df)
```

### Make a plot showing the proportion of null-rejected

```{r}
sim_results_mu6 %>% 
  group_by(mu_value) %>% 
  summarize(
    total = n(),
    null_reject = sum(p.value < 0.05)) %>% 
  mutate(
    proportion = null_reject / total
  ) %>%
  ggplot(aes(x = mu_value, y = proportion)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Proportion of null hypothesis rejected for each μ value",
    x = "True value of μ "
  ) +
  scale_x_continuous(
    breaks = 1:6
  )
```

**Description of the association between effect size and power**

The plot shows that as the true value of μ increases, the proportion of null hypothesis rejected increases, that is, the power increases. As mentioned in the null hypothesis, the mean equals 0, so as the true mean increases, the effect size increases. Therefore, the larger the effect size, the more power the test has.





